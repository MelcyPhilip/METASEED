---
title: "METASEED"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
.bash {background-color:  #CCCCCC; }
.bash {color: #000044;}
.cmd {background-color: #333333;}
.cmd {color: #EEEEEE;}
</style>


## What is this?
The METASEED is a software pipeline for reconstructing 16S sequences from a microbial community. The METASEED is meant to be used for environmental data, where we start out with amplicon sequencing, and we find many of the amplicon sequences come from organisms we have not seen before, and where we have no full-length 16S in the databases. Then we use Whole Genome Sequencing (WGS) metagenome data from the same samples to reconstruct these 16S sequences.

Other excellent tools like EMIRGE or MATAM use only the WGS reads (no amplicon data) and depend on having a database of already known full-length 16S, and use this to identify the corresponding WGS reads. This works fine for data from well studied environments (e.g. the human gut) where virtually all 16S variants have been seen before. But they will not be able to discover new organisms not already in this database. This is the exact purpose of METASEED, to be able to get an idea about the full-length version of a 16S we have never seen before.

It is built on the following idea:

1. Use Illumina 16S amplicon data from the community to find the OTUs (or ASVs) using some standard approach. The obtained high-quality amplicon sequences for the abundant OTUs are the *seeds* of the procedure.
2. Do a full metagenome (WGS) sequencing on the same samples. We assume high precision Illumina paired-end data also here.
3. Starting with the shorter amplicon seeds (usually around 400 bases long), the METASEED collects read-pairs from the WGS data matching these, and from this *grow* them into longer 16S sequences. 



## This folder
In this folder you find code and some data necessary to run the METASEED and see how it works.

The code is UNIX shell (bash) and R code that you find inside the files

* `METASEED_main.sh`
* `read_collecting.sh`
* `assembly.sh`

The dependent software is found as `apptainer` (used to be `singularity`) containers in the folder `containers`. Thus, you need to have `apptainer` on your system to use this. You also need `R` since the scripts contain a lot of `R` code.

A small data set is found in the folder `example`.


## Running METASEED - the `METASEED_main.sh`
The shell script `METASEED_main.sh` is what you run to execute this pipeline. From the command line simply:
```{.cmd}
./METASEED_main.sh
```
It is by default set up to find the containers inside the folder `containers` and the data inside `example`. We will now step through this script to explain how you may edit this to suit your own world.


### Running on a High Performance Computing cluster
We use this on a High Performance Computing (HPC) cluster, and the `#SBATCH` lines at the beginning are settings for the SLURM system. Ignore these (or delete them) if you run this on a single computer.


### Settings
The next section is where you typically edit:
```{.bash}
###############
### Settings
##
seed_sequence_file="example/seed_sequences.fasta"   # fasta file with the seeds (=amplicon sequences)
seed_abundance_file="example/seed_abundances.tsv"   # text file with abundances for each seed
sample_table_file="example/sample_data.tsv"         # text file with metadat for each sample
tmp_folder="tmp"                                    # folder for temporary output
out_folder="out"                                    # folder for final results
threads=10                                          # number of threads
dbase=$tmp_folder/METASEED                          # the bowtie2 and BLASt database prefix
max_n=3                                             # maximum number of seeds a read can match
```
The `seed_sequence_file` is the name of the fasta-file with the seed sequences. These are the OTU sequences from your amplicon data. The header-line for each seed sequence must start with a text unique to each seed, we call it he `seed_id`. In the example data these are `OTU1`, `OTU2`,...etc. These must be the first token. Add a space and more text if you like, as long the first token is this identifier.

Note that the more seed sequences you have, the longer time this procedure takes. The very low-abundant OTUs will never get a reconstructed sequences due to too few WGS reads matching them. Therefore, you loose little by including only the more abundant OTUs as seeds.

The `seed_abundance_file` is the name of the tab-separated text file with abundances for the seeds, from the amplicon data. This is typically the read count table you get from all amplicon data analysis pipelines. The first column lists the `seed_id`s, the exact same identifiers as in the `seed_sequence_file`, which is required to join the information from these two files. Then follows columns with abundances for the seeds in a number of *samples*, one column for each sample. These are the same samples that you also have WGS data for (see below). Thus, the column names are the `sample_id` identifiers unique to each sample. In the `example` data they are `sample_1` and `sample_2` (only 2 samples). Note that the abundances are only used for ranking the OTUs inside each sample, and raw read counts as well as relative abundances work equally well.

The `sample_table_file` is also a tab-separated text file with 3 columns. The first column `sample_id` lists the texts used as column names in the `seed_abundance_file`, see above. The other two columns, named `R1_file` and `R2_file`, list the fastq files for the WGS data (not the amplicon data!). Additional columns may be present, but these three columns are required, and must have these exact column names.

When running METASEED on your own data, these three files are what you need to prepare. See the `example` data for more on how they are formatted.

The `tmp_folder` is where temporary output will be stored during computations. Nothing in there will be deleted until you do it. The `out_folder` will receive the final results. The number of `threads` to use you decide. The `dbase`is simply the prefix for the `bowtie2` and `BLAST` temporary databases, no need to change this. The `max_n` is the maximum number of seeds a read is allowed to match. WGS reads that matches 'all' kinds of seeds are difficult to trust. We have found 3 is a good balance between too strict (1) and too liberal (>>3), see our paper for details.


### Containers
The pipeline uses external software:

* `bowtie2` for mapping WGS reads to seeds.
* `samtools` for collecting the mapped reads.
* `blast` for aligning the collected reads to get more precise alignment information.
* `bbmap` for collecting the final read-pairs from the WGS data.
* `spades` for assembly of the reads for each seed.

The commands to execute the containers are specified in `METASEED_main.sh`, e.g. 

```{.bash}
export bowtie2_app="apptainer exec containers/bowtie2:2.5.0--py39h3321a2d_0.sif"
```
If you put the containers elsewhere, you need to edit the corresponding path here. These shell variables are exported since they are used by two other scripts as well. They could also be transferred as arguments instead of being exported.

The versions of the current containers are seen from the container file names (e.g. version 2.5.0 for `bowtie2` above). Feel free to replace them with other software versions, and edit the code above accordingly.


### The three steps
The main part of this script is divided into 3 steps:

1. Building the databases. This is a small step.
2. Collecting the WGS reads. This is a BIG part of this procedure, since it involves the WGS data, which is often large fastq files.
3. The assembly. There is one assembly for each seed, so its duration depends on the number of seeds. However, the number of reads to assemble for each seed is usually small, so this is very much faster than any genome assembly.

In the `METASEED_main.sh` you will see that step 2 involves looping over all the samples. This is fine as long as the WGS data sets are smaller and not too many. For large WGS data sets (huge R1 and R2 fastq files) this takes time. On our HPC cluster we therefore make use of array jobs to run this on all samples in parallel instead of the sequential looping. To do this, you need to modify the existing `METASEED_main.sh`, we just mention it for those who might see the need and have such facilities available.



## Additional scripts
Inside the `METASEED_main.sh` you will find it calls upon the two other scripts:

* `read_collecting.sh`
* `assembly.sh`

In principle there should be no need to edit any of these two scripts. They contain bash and R code that is 'hidden away' in these two scripts simply because including it all in the main script would make the latter large and more difficult to overview.



## The data
The folder `example/` contains some small data just to be able to run the pipeline for testing. To use this on your own data, you need to understand the content of this folder. You always need three files. Edit the `METASEED_main.sh` section Settings (see above) if you give them other names or locations.

There are three files you need to replace:

### `seed_sequences.fasta`
This fasta file simply contains the OTU (or ASV) sequences that you want to use as seeds, i.e. you want to grow longer versions of these 16S sequences. You typically get this from some 16S data processing procedure. Note that the first token in the header-lines of this fasta file must be the `seed_id` text that uniquely identifies each seed. This will be used in some file names, and must therefore be one contiguous text, i.e. no spaces or special characters inside. In the `example` data these are `OTU1`, `OTU2`,...etc. Here are the two first entries of the example file:
```{.bash}
>OTU1
CCTACGGGAGGCAGCAGTGGGGAATCTTAGACAATGGGCGAAAGCCTGATCTAGCCATGCCGCGTGAGTGATGAAGGTCT
TAGGATCGTAAAGCTCTTTCGCCAGGGATGATAATGACAGTACCTGGTAAAGAAACCCCGGCTAACTCCGTGCCAGCAGC
CGCGGTAATACGGAGGGGGTTAGCGTTGTTCGGAATTACTGGGCGTAAAGCGTACGTAGGCGGATTAGAAAGTATGGGGT
GAAATCCCAGGGCTCAACCCTGGAACTGCCTCATAAACTACTAGTCTAGAGTTCGAGAGAGGTGAGTGGAACTCCGAGTG
TAGAGGTGAAATTCGTAGATATTCGGAAGAACACCAGTGGCGAAGGCGGCTCACTGGCTCGATACTGACGCTGAGGTACG
AAAGTGTGGGGAGCAAACAGGATTAGATACCCTGGTAGTCC
>OTU10
CCTACGGGAGGCAGCAGTGGGGAATATTGGACAATGGACTAAAAGTCTGATCCAGCAATTCTGTGTGCACGATGAAGGTC
TTCGGATCGTAAAGTGCTTTCAGGTGGGAAGAAGAAAGTGACGGTACCACCAGAAGAAGCGACGGCTAAATACGTGCCAG
CAGCCGCGGTAATACGTATGTCGCAAGCGTTATCCGGAATTATTGGGCGTAAAGCGCGTCTAGGCGGCCTTTTAAGTCTG
ATGTGAAAATGCGGGGCTCAACTCCGTATTGCGTTGGAAACTGGAAGGCTAGAGTATCAGAGAGGTGGGCGGAACTACAA
GTGTAGAGGTGAAATTCGTAGATATTTGTAGGAATGCCGATGGGGAAGCCAGCTCACTGGATGAATACTGACGCTAAAGC
GCGAAAGCGTGGGGAGCAAACGGGATTAGATACCCCGGTAGTCC
```


### `seed_abundances.tsv`
This is a tab-separated text file with a table having one row for each seed. The first column lists the `seed_id`s mentioned above. The remaining columns list the abundances for each seed in each sample. The column names (except the first column) must be the `sample_id`s, i.e. unique texts identifying the samples you have WGS data for. Again, these texts will be used in file names, and must be one contiguous text without spaces or special characters. In the `example` data these are `sample_1` and `sample_2`. You may of course have many more samples. Here are the first lines of the example file:
```{.bash}
OTU	        sample_1    sample_2
OTU1        3810        0
OTU10       7           0
OTU100      67          133
OTU101      72          0
OTU102      66          0
```


### `sample_data.tsv`
A tab-separated file with a table having one row for each sample. The first column must be named `sample_id`, and lists these identifier texts for each sample, mentioned above. The columns named `R1_file` and `R2_file` then lists the corresponding fastq files (paired end data) for each sample. Note that the columns *must be named exactly as specified above* in order to be read properly in `METASEED_main.sh`. Here is how the small table looks like in the example data:
```{.bash}
sample_id   R1_file                         R2_file
sample_1    example/sample_1_R1.fastq.gz    example/sample_1_R2.fastq.gz
sample_2    example/sample_2_R1.fastq.gz    example/sample_2_R2.fastq.gz
```


### WGS data
In the `example` folder you also find the WGS fastq file pairs, but these you may of course have anywhere you prefer. Just edit the path/name of them in the `sample_data.tsv` file (or whatever you call it).




## Output
The output will be in the folder you specify as `out` in the `METASEED_main.sh`. There will be a fasta file for each seed, containing the reconstructed sequence. Note that some seeds may not be reconstructed! If too few WGS reads are collected for some seed, the assembly will fail, and there is no result.

After reconstruction, the sequence is BLASTed against the original amplicon seeds, and in the header line is listed which seed gave the best match to the reconstructed, and how good (identity). Of course this should be the same as the seed it was grown from! This is not always the case, even if the original seed is used as a *trusted contig* in `spades`. If two seeds are quite similar, and rather few and/or bad WGS reads are collected, then the reconstructed sequence may actually diverge so much from the seed that it matches another seed better. Such results you may then discard as poor reconstructions.





